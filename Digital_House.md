# MÓDULO 0:
## Fundamentos: NumPy, Stats, SQL e DataViz
• Python e Numpy: demonstrar conceitos de programação usando as ferramentas Python e NumPy para tratar bases de dados e DataFrames.  
• Estatística descritiva: rever e aplicar os fundamentos de estatística descritiva.  
• SQL e Bancos de Dados: conhecer diferentes tipos de bancos de dados, rever comandos em SQL e realizar atividades práticas, obtendo dados de bancos de dados.  
• Introdução a Gráficos e Visualização: Realizar atividades práticas usando notebooks Jupyter com biblioteca plotly e matplotlib para visualização de dados.  

# MÓDULO 1:
## Exploratory Data Analysis (EDA), Pandas e SciPy
• Pandas e Tabelas Pivot: introdução a Pandas (biblioteca para ler, limpar, realizar o parsing e representar por gráficos os dados usando funções booleanas, indexação, séries, joins e outras funcionalidades).  
• Limpeza de dados: conceitos de “tidy data”, estruturação de dados, introdução ao problema de dados faltantes e noções de expressões regulares.  
• EDA: Análise Exploratória de Dados em Pandas, com visualização de insights sobre recursos de dados  
• Estatística Inferencial: Noções de probabilidade, distribuições amostrais, intervalos de confiança, testes de hipótese. Uso de biblioteca SciPy.  

# MÓDULO 2:
## Introdução a Regressão Linear, Modelagem Estatística, Regressão Logística, Statsmodels e Scikit-Learn
• Regressão Linear Simples e Múltipla: estimativa e interpretação dos coeficientes, suposições, medidas de ajuste. Introdução a não linearidades no modelo.  
• Regressão Logística: princípio de transformação matemática sobre a regressão linear, interpretação dos estimadores, introdução a métricas para modelos de classificação  
• Modelagem Estatística: análise Exploratória de dados e feature engineering com foco no aumento da explicabilidade dos dados  
• Statsmodels e Scikit-Learn: introdução ao uso das duas bibliotecas para estimar modelos estatísticos e algoritmos de Machine Learning em Python. API, tipos e objetos na Scikit-Learn.  

# MÓDULO 3:
## SQL, Web Scrapping, Consumindo APIs, Introdução ao Machine Learning, Compressão e Clusterização de dados
• Machine Learning: introdução a conceitos nodais: desvio, variância, overfitting, underfitting. Identificação de diferentes tipos de algoritmos de Machine Learning (supervisionados e não supervisionados). Formas de estimar o erro de generalização (train-test split, cross-validation). Apresentação geral das técnicas mais utilizadas.  
• APIs e Web Scraping: Selenium e Beautiful Soup para raspar sites, e estrutura de dados JSON para uso de APIs  

# MÓDULO 4:
## Classificação, árvores de Decisão e regularização
• Algoritmos usados para problemas de classificação: Regressão Logística, Naïve Bayes, Support Vector Machines (SVM). Avaliação de algoritmos de classificação: métricas de erro, acurácia e limitações da acurácia. Precisão, recall, F1 score, curva ROC, área sob a curva (AUC).  
• Árvores de Decisão: construção de árvores por meio do algoritmo CART, evitando overfitting e underfitting, passando pelo mecanismo de separação de dados utilizando as estratégias de entropia de gini.  
• Regularização: noções de regularização. Regressão LASSO e Ridge. Feature scaling.  

# MÓDULO 5:
## Ensembles e sistemas de recomendação
• Modelos de Ensemble: Noção de Ensemble Learning. Vantagens e desvantagens. Apresentação do meta-algoritmo Bagging e de duas aplicações para árvores de decisão: Random Forest e ExtraTrees. Apresentação do meta-algoritmo de Boosting e de AdaBoost e Gradient Boosting. Entrando na caixa preta: feature importance e partial dependence plot em modelos de ensemble.  
• Sistemas de Recomendação: técnicas de filtragem de dados e operações com matrizes para relativizar a proximidade de pontos que sejam clientes ou produtos, como base de um sistema de recomendação.  

# MÓDULO 6:
## Criação de APIs, Deploy de Modelos, Séries Temporais e interpretação de Modelos
• Criação de APIs: Utilização de ferramentas de Deploy e configuração em URL pública.  
• Deploy de Modelos: Pipelines: automatização e encapsulamento de etapas do workflow. Deploy de modelos de machine learning em ambiente Web  
• Séries Temporais: Pré-processamento de séries. Enfoque clássico (tendência, ciclo e resíduo). Sazonalidade. Modelos básicos: média constante, tendência determinística, média móvel, suavização exponencial simples. Modelos avançados: processos AR, MA, ARMA, ARIMA e ARIMAX.  
• Interpretabilidade de Modelos: Utilização de pacotes especiais do Python como Shap e Lime para interpretação de modelos caixa preta, que não oferecem uma possibilidade de entendimento sobre explicabildade do modelo.  

# MÓDULO 7:
## Agile, Ética e Privacidade e Apresentação do Projeto Integrador
• Agile: metodologia de gerenciamento de projeto de programação com curtos e testes frequentes para agilidade na execução de tasks, com trabalho em equipe.  
• Ética e Privacidade: Apresentação dos desafios da Ciência de Dados na sociedade atual, e análise crítica da LGPD (Lei Geral de Proteção de Dados) com as permissões de utilização de dados, preservação de direitos e o que fazer para se adequar à legislação evitando tomada de risco empresarial  
• Projeto Integrador: projeto de Data Science a ser executado pelos alunos, divididos em grupos de 5 a 7 participantes, contendo todas as etapas de um trabalho semelhante ao encontrado na vida real das empresas. Os professores caminham junto com os alunos durante o processo de construção da solução até a apresentação final  
